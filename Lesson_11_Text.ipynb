{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lesson 11: Learning from Text\n",
    "====================\n",
    "---\n",
    "Prof. James Sharpnack<br>\n",
    "Statistics Department, UC Davis<br>\n",
    "&copy; 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural language processing basics\n",
    "\n",
    "We will begin with some basics of natural language processing.  A text is just a long string of characters, and a corpus is a collection of text documents.  For example, we can have a corpus of news articles, journal paper abstracts, etc.  We have discussed how strings can have different encodings, such as plain ascii, extended ascii, Unicode, etc.  This is how the bytes are converted into characters and vice versa.  In most written languages, words are separated by spaces and punctuation, but in a typical text these words will be mixed with numbers, abbreviations, etc.  So, we will generalize the notion of word to include every basic unit of characters that are separated by whitespace.  These generalized words are called tokens, and the process of making text into a list of tokens is tokenization.  In nltk, you can use <code>nltk.word_tokenize()</code> to convert text into a list of tokens.  This essentially splits the string by whitespace, but also does necessary stripping.  A vocabulary is the list of tokens in a document or corpus regardless of token order.  For example, <code>vocab = set(token_list)</code> will convert a list, <code>token_list</code>, into a set, which is unordered.  The most general and assumption-free way to think about a text document is as a list of tokens.\n",
    "\n",
    "**Checkpoint:** read through <a href=\"http://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html\">this description of stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other parts of our text processing pipeline are the removal of stop words, stemming, lemmatization, and n-gram extraction.  With each transformation you are implicitly making another assumption, and losing generality, so you should think about each transformation carefully.  Stop words are words that are so common (i.e. a, an, are, have, etc.) that they are removed from the analysis.  Typically, one constructs a list of stop words from the most common words in a corpus, or from a predefined list of stop words.  Stemming and lemmatization are ways to identify derivationally related forms of the same word.  So, for example, the words, derivative, derivation, deriving, derived are all derivatives of the word derive.  In many circumstances, you do not want to make a distinction between these forms and convert them into the same word, which we will call a lemma.  The process of lemmatization is how to convert a token into a lemma.  So for example, <code>lemmatizer('derivative') = 'deriv'</code> would be an example of such a function, which is naturally a many-to-one function (also known as a surjection).  Stemmers are more crude lemmatizers that heuristically remove stems of various common forms, for example they may remove the 'ation' from 'derivation'.  A popular algorithm is the Porter stemmer.  More advanced lemmatizers will use morphological analysis to lemmatize more precisely.  These will use parts-of-speech in order to include context into the resulting lemmata.  We will discuss n-grams in the bag-of-words models section.\n",
    "\n",
    "**Checkpoint:** Read through <a href=\"http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\">this description of stemming and lemmatization.</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification and Information Retrieval\n",
    "\n",
    "Information retrieval is the basic problem of retrieving text documents based on some criteria.  We will specifically consider situations in which a search text is matched with documents in some corpus.  So if I submit 'ballad of dorothy' to corpus containing all of the song titles on spotify then it will return a list of songs (hopefully, Prince's 'Ballad of Dorothy Parker' will be a the top of the list).  If the returned list is ordered from most relevant to least, then this is called a ranking.  We will assume that we are tasked with the problem of returning a ranked list of text documents from a input string.  There are many ways to evaluate the performance of a ranking, but typically it is through the use of labels that come from some moderating human.  A simple way to rank documents is through a similarity measure between documents, where you return the documents that have the most similarity to the input string first.  This requires such a similarity measure, and typically this will be designed based on the context, although we will go over some common similarity measures.\n",
    "\n",
    "A related problem is that of classification.  Classifiers are prediction algorithms that answer yes/no questions.  For example, is this bread moldy or not? Does this tissue sample contain tumor cells? Will this customer default on their loan? Will I get an A in 141B? To answer such questions, one needs some additional information, and context, and we will call these the features of the samples.  For example, we could use a photograph of the bread to detect mold, we could use the result of high-throughput gene sequencing to detect tumor cells, we could use the credit history of the customer to predict if they will default, and we could use your current grades to predict if you will get an A.  Training is the process by which any loose parameters in the classifier are specified through the use of data.  This data must be labelled for this to make sense, where labels are the values that you would like to predict.  So if we have some examples of tumor and normal cells then maybe we can train a classifier to classify a future tissue sample.\n",
    "\n",
    "For example, suppose you are given a document and are asked if it is a IRS tax publication or not.  You may specify a few words, such as 'income', 'IRS', 'deductible', etc. and say that if it contains any of these then it is.  This may work well, but there may be a better way to do this.  You could give each word in a list a score and then you add that word score to a document score if it contains the word.  This is a form of regression, whereby each word is a new feature that has a regression coefficient associated with it.  So training a classifier of this type would be setting these regression coefficients through labelled data.  We will focus instead on nearest-neighbors classifiers that use a similarity measure to classify new documents from labelled documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-words Model\n",
    "\n",
    "So far, we have a number of transformations that will convert a document to a list of atomic units, i.e. tokens, stems, lemmata.  Without loss of generality, we will assume that you have converted your document into lemmata, since you could just use the identity transformation as a lemmatizer.  If you were given this list then you could reconstruct the order of these lemmata, and see that 'pink' is followed by 'floyd'.  In the bag-of-words model, we will make the ridiculous assumption that the order does not matter.  Implicitly, we are making the modelling assumption that a document is generated by the author selecting words at random from a bag of words that the document contains and writing them in order.  For example, 'My dog ran after the ball' is the same thing as 'My ball after the dog ran', since both are equally likely.  While this assumption is clearly not true, the resulting methods empirically can have quite good performance for some problems.  This is an extreme example of George Box's adage, 'All models are wrong, but some models are useful.'\n",
    "\n",
    "A slightly more refined variation on the bag of words model is the bag of n-grams model, by which we consider n-tuples of lemmas, called n-grams, as the atomic unit.  For example, if we take the string 'All models are wrong, but some are useful', then tokenize and stem it obtaining <code>['all', 'model', 'are', 'wrong', 'but', 'some', 'are', 'use']</code> then the bigrams are: <br>\n",
    "<pre>('all', 'model'),\n",
    " ('model', 'are'),\n",
    " ('are', 'wrong'),\n",
    " ('wrong', 'but'),\n",
    " ('but', 'some'),\n",
    " ('some', 'are'),\n",
    " ('are', 'use').</pre>\n",
    "Then typically, the order of the bigrams is ignored, so that this is the same as<br>\n",
    "<pre>('are', 'wrong'),\n",
    " ('but', 'some'),\n",
    " ('all', 'model'),\n",
    " ('model', 'are'),\n",
    " ('are', 'use')\n",
    " ('wrong', 'but'),\n",
    " ('some', 'are').</pre>\n",
    "This way, we preserve some word order information, but not all of it.  Similarities based on the bag of words or bag of n-grams models typically compare the lemmas and n-grams in common between two documents regardless of where they appear in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing a document\n",
    "\n",
    "The simplest way to construct a similarity measure is by counting the number of times the same lemma occurs in two documents.  So, the similarity between \"Ziggy Stardust and the spiders from Mars\" and \"Is there life on Mars?\" have a similarity of 1, in this metric, because they share only one word, \"Mars\".  The disadvantage of this is that it weights rare words and common words in the same way.  We will think about how we can vectorize a tokenized (or lemmatized) document.  To vectorize a document, we will think about constructing a feature which is the count of each lemma in the vocabulary.  So if our vocabulary (after the removal of stop words and tokenization/lemmatization) is ['dog','hot','flick','banana','explanation'] then the sentence 'That hot dog requires an explanation' is vectorized as <code>[1,1,0,0,1]</code>, and the sentence 'give me an explanation for this banana flick' is vectorized as <code>[0,0,1,1,1]</code>.  So we can then compute the dot product between these vectors as <code>0(1) + 0(1) + 1(0) + 1(0) + 1(1)</code> which is the sum of the product of their features.\n",
    "\n",
    "Alternatively, we can weight the counts by the inverse document frequency (idf), so that each feature product is weighted by <code>log(N / (1 + n_docs[lemma]))</code> where n_doc is the number of documents containing lemma and N is the total number of documents (this particular specification is called the smoothed-idf).  We will use <code>sklearn.feature_extraction.text.TfidfVectorizer</code> to perform this vectorization in the example below.  The TfidfVectorizer will return a sparse matrix, which is a matrix that is stored in a clever way in that it does not store the 0's in the matrix.  This not only frees up space in memory, but speeds up matrix computations.  We will use <code>sklearn.neighbors.NearestNeighbors</code> to compute the dot-products after vectorization and determine the nearest neighbors by returning the samples that have the largest similarities to any sample.\n",
    "  We can use the same ideas to do this with n-grams, by thinking of n-grams as our lemmas instead of individual tokens."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
